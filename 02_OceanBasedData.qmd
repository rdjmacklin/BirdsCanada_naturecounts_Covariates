---
title: "Data Extraction"
author: "Rory Macklin (rmacklin@birdscanada.org)"
format: html
editor: visual
---

# Goal

In this document, we aim to demonstrate how a Nature Counts user may pull Information from various R packages interfacing with spatial datasets and pair them to datasets from Nature Counts. We demonstrate this using data from the British Columbia Coastal Waterbird Survey which can be accessed via <https://www.doi.org/10.71842/1azg-s486>. This is part of Birds Canada's commitment incorporate satellite earth observation data download into the naturecounts R package.

# Code

## Setup

Upon downloading this repository, execute the included setup script to create necessary directories and load required packages.

```{r}

source("./Scripts/00_Setup.R")

nc_login <- readline(prompt = "Enter Nature Counts username: ")
nc_reqid <- as.numeric(readline(prompt = "Enter Nature Counts request id: "))

```

## Downloading naturecounts data.

Here, we download data from the BC Marsh Monitoring Program using the provided Nature Counts login info. We will convert this object into a spatial features (sf) object with each point buffered to a radial distance of 500m (the user can adjust this by modifying the *radius* object below).

```{r}

if(!file.exists("./Data/Raw/bccws.csv")) {
  
  dat <- nc_data_dl(username = nc_login, request_id = nc_reqid) %>%
    filter(survey_year %in% c(2021:2025))

  write_csv(dat, "./Data/Raw/bccws.csv")
  
} else {
  
  dat <- read_csv("./Data/Raw/bccws.csv")
  
}

dat$latitude[dat$SiteCode == "SCBI-10"] <- 49.34003
dat$longitude[dat$SiteCode == "SCBI-10"] <- -123.347

dat_sf <- dat %>%
  filter(!(is.na(latitude) | is.na(longitude))) %>%
  select(SiteCode, latitude, longitude, survey_year, survey_month, survey_day) %>%
  distinct() %>%
  arrange(SiteCode, survey_year, survey_month) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>%
  st_transform("ESRI:102001")

radius <- 500

dat_buff <- st_buffer(dat_sf, radius)

study_area <- st_bbox(dat_sf) %>%
  st_as_sfc() %>%
  st_buffer(2500) %>%
  st_transform(4326)

rm(dat)

```

## Temperature, salinity, productivity data via *biooracler*

```{r}

if(!dir.exists("./Data/Raw/biooracle")) {
  
  dir.create("./Data/Raw/biooracle")
  
}

# See list_layers() for layer names and options

biooracle_vars <- list(SST = "thetao_baseline_2000_2019_depthsurf", # Surface temperature, averaged from 2000-2019.
                       Salinity = "so_baseline_2000_2019_depthsurf", # Salinity, averaged from 2000-2019.
                       SeaWaterSpeed = "sws_baseline_2000_2019_depthsurf", # Sea water speed, averaged from 2000-2019.
                       Chlorophyll = "chl_baseline_2000_2018_depthmean") # Chlorophyll, averaged from 2000-2018.

constraints <- list(time = c('2010-01-01T00:00:00Z', '2010-01-01T00:00:00Z'),
                    latitude = c(st_bbox(study_area)$ymin,
                                 st_bbox(study_area)$ymax),
                    longitude = c(st_bbox(study_area)$xmin,
                                  st_bbox(study_area)$xmax))

for(i in names(biooracle_vars)) {
  
  var_grab <- paste0(str_extract(biooracle_vars[[i]], "[^_]+"), "_mean")
  
  biooracle <- download_layers(dataset_id = biooracle_vars[[i]],
                               variables = var_grab, 
                               constraints = constraints,
                               fmt = "raster",
                               directory = "./Data/Raw/biooracle")
  
  for(j in unique(dat_sf$SiteCode)) {
  
    tmp <- dat_buff %>%
      filter(SiteCode == j) %>%
      select(SiteCode, geometry) %>%
      distinct() %>%
      st_transform(crs(biooracle))
  
    dat_sf[dat_sf$SiteCode == j, i] <- exact_extract(x = biooracle,
                                                            y = tmp,
                                                            fun = "mean",
                                                            progress = FALSE)
    
    if(is.nan(unique(dat_sf[dat_sf$SiteCode == j, i] %>% st_drop_geometry() %>% unlist() %>% unname()))) {
      
      tmp <- dat_sf %>%
        filter(SiteCode == j) %>%
        select(SiteCode, geometry) %>%
        distinct() %>%
        st_transform(crs(biooracle))
      
      rast_to_point <- as.points(biooracle) %>%
        st_as_sf()
      
      dat_sf[dat_sf$SiteCode == j, i] <- rast_to_point[st_nearest_feature(tmp, rast_to_point), var_grab] %>% st_drop_geometry() %>% unlist() %>% unname()
      
      rm(rast_to_point)
      
      }
  
    }
  
  rm(var_grab)
  rm(biooracle)
  rm(tmp)
  rm(j)
  
}

rm(constraints)
rm(i)

```

## Bathymetry data via *marmap*

```{r}

if(!dir.exists("./Data/Raw/bathy")) {
  
  dir.create("./Data/Raw/bathy")
  
}

bathy <- getNOAA.bathy(lon1 = st_bbox(study_area)$xmin,
              lon2 = st_bbox(study_area)$xmax,
              lat1 = st_bbox(study_area)$ymin,
              lat2 = st_bbox(study_area)$ymax,
              resolution = 0.25,
              keep  = TRUE,
              path = "./Data/Raw/bathy")

bathy <- marmap::as.raster(bathy) %>%
  rast()

bathy[bathy > 0] <- NA

for(i in unique(dat_sf$SiteCode)) {
  
    tmp <- dat_buff %>%
      filter(SiteCode == i) %>%
      select(SiteCode, geometry) %>%
      distinct() %>%
      st_transform(crs(bathy))
  
    dat_sf[dat_sf$SiteCode == i, "bathy"] <- exact_extract(x = bathy,
                                                           y = tmp,
                                                           fun = "mean",
                                                           progress = FALSE)
  
    }

rm(i)
rm(tmp)
rm(bathy)

```

## Landcover data via *geodata*

Data from [ESA Worldcover](https://esa-worldcover.org/en) at a 10-minute resolution, derived from imagery from 2020/2021. Looking for a better time-series option but availability seems limited - MODIS functionality is sketchy as server access appears inconsistent.

```{r}

lc_vars <- c("trees", "cropland", "built", "water", "wetland") # other options include "grassland", "bare", "snow", "mangroves", "moss".

for(i in lc_vars) {
  
  landcover(var = i, path = paste0("./Data/Raw"))
  
  lc <- rast(paste0("./Data/Raw/landuse/WorldCover_", i, "_30s.tif"))
  
  lc <- crop(lc, project(study_area, crs(lc))) %>%
    project(crs(dat_buff))
  
  for(j in unique(dat_sf$SiteCode)) {
    
      tmp <- dat_buff %>%
        filter(SiteCode == j) %>%
        select(SiteCode, geometry) %>%
        distinct()

      dat_sf[dat_sf$SiteCode == j, paste0("lc_",i)] <- exact_extract(x = lc,
                                                                     y = tmp,
                                                                     fun = "mean",
                                                                     progress = FALSE)
    
      rm(tmp)
  }
  
  rm(lc)
  
}

rm(i)
rm(j)

```
